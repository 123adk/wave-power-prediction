"""Time-series Generative Adversarial Networks (TimeGAN) Codebase - PyTorch Implementation

Reference: Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar,
"Time-series Generative Adversarial Networks,"
Neural Information Processing Systems (NeurIPS), 2019.

Paper link: https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks

Last updated Date: 2025-10-17
Converted to PyTorch by: GitHub Copilot

-----------------------------

predictive_metrics.py (PyTorch Version)

Note: Use Post-hoc RNN to predict one-step ahead (last feature)
"""

# Necessary Packages
import torch
import torch.nn as nn
import numpy as np
from sklearn.metrics import mean_absolute_error
from utils import extract_time


class Predictor(nn.Module):
    """Simple predictor network."""

    def __init__(self, input_dim, hidden_dim):
        super(Predictor, self).__init__()
        self.rnn = nn.GRU(input_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x, lengths):
        # Pack padded sequence
        packed_input = nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)
        packed_output, _ = self.rnn(packed_input)
        output, _ = nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)

        # Apply fully connected layer
        y_hat = self.sigmoid(self.fc(output))
        return y_hat


def predictive_score_metrics(ori_data, generated_data):
    """Report the performance of Post-hoc RNN one-step ahead prediction.

    Args:
        - ori_data: original data (åŸå§‹å°ºåº¦ï¼Œæœªå½’ä¸€åŒ–)
        - generated_data: generated synthetic data (åŸå§‹å°ºåº¦ï¼Œæœªå½’ä¸€åŒ–)

    Returns:
        - predictive_score: MAE of the predictions on the original data
    """

    # ========== æ·»åŠ æ•°æ®éªŒè¯ ==========
    print(f'\nğŸ” Predictive Metrics - æ•°æ®æ£€æŸ¥:')

    # æ£€æŸ¥æ•°æ®èŒƒå›´
    ori_array = np.array([d for d in ori_data])
    gen_array = np.array([d for d in generated_data])

    print(f'  åŸå§‹æ•°æ®èŒƒå›´: [{ori_array.min():.4f}, {ori_array.max():.4f}]')
    print(f'  ç”Ÿæˆæ•°æ®èŒƒå›´: [{gen_array.min():.4f}, {gen_array.max():.4f}]')
    print(f'  åŸå§‹æ•°æ®å‡å€¼: {ori_array.mean():.4f}')
    print(f'  ç”Ÿæˆæ•°æ®å‡å€¼: {gen_array.mean():.4f}')

    # æ£€æŸ¥å¼‚å¸¸å€¼
    if np.isnan(gen_array).any() or np.isinf(gen_array).any():
        print(f'  âŒ é”™è¯¯: ç”Ÿæˆæ•°æ®åŒ…å« NaN æˆ– Infï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°!')
        return float('inf')

    if abs(gen_array.max()) > 1e8 or abs(gen_array.min()) > 1e8:
        print(f'  âš ï¸  è­¦å‘Š: ç”Ÿæˆæ•°æ®æ•°å€¼å¼‚å¸¸ï¼Œè¯„ä¼°ç»“æœå¯èƒ½ä¸å‡†ç¡®!')

    # Basic Parameters
    no, seq_len, dim = np.asarray(ori_data).shape

    # Set maximum sequence length and each sequence length
    ori_time, ori_max_seq_len = extract_time(ori_data)
    generated_time, generated_max_seq_len = extract_time(generated_data)
    max_seq_len = max([ori_max_seq_len, generated_max_seq_len])

    # Network parameters
    hidden_dim = int(dim / 2)
    iterations = 5000
    batch_size = 128

    # Device configuration
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # ========== å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼ˆç”¨äºè®­ç»ƒé¢„æµ‹å™¨ï¼‰==========
    # æ³¨æ„: è¿™é‡Œéœ€è¦å½’ä¸€åŒ–ï¼Œå› ä¸ºç¥ç»ç½‘ç»œè®­ç»ƒéœ€è¦æ ‡å‡†åŒ–çš„è¾“å…¥
    def normalize_data(data):
        """å½’ä¸€åŒ–æ•°æ®ç”¨äºç¥ç»ç½‘ç»œè®­ç»ƒ"""
        data_array = np.array([d for d in data])
        min_val = data_array.min(axis=(0, 1))
        max_val = data_array.max(axis=(0, 1))

        normalized = []
        for d in data:
            norm_d = (d - min_val) / (max_val - min_val + 1e-7)
            normalized.append(norm_d)

        return normalized, min_val, max_val

    # å½’ä¸€åŒ–è®­ç»ƒæ•°æ®ï¼ˆgenerated_dataï¼‰
    generated_data_norm, gen_min, gen_max = normalize_data(generated_data)

    # å½’ä¸€åŒ–æµ‹è¯•æ•°æ®ï¼ˆori_dataï¼‰- ä½¿ç”¨ç›¸åŒçš„å½’ä¸€åŒ–å‚æ•°
    ori_data_norm = []
    for d in ori_data:
        norm_d = (d - gen_min) / (gen_max - gen_min + 1e-7)
        ori_data_norm.append(norm_d)

    print(f'  âœ… æ•°æ®å·²å½’ä¸€åŒ–ç”¨äºé¢„æµ‹å™¨è®­ç»ƒ')

    # Build predictor
    predictor = Predictor(dim - 1, hidden_dim).to(device)

    # Loss and optimizer
    criterion = nn.L1Loss()
    optimizer = torch.optim.Adam(predictor.parameters())

    # Training using Synthetic dataset (ä½¿ç”¨å½’ä¸€åŒ–çš„æ•°æ®)
    predictor.train()
    for itt in range(iterations):
        # Set mini-batch
        idx = np.random.permutation(len(generated_data_norm))
        train_idx = idx[:batch_size]

        X_mb = list(generated_data_norm[i][:-1, :(dim - 1)] for i in train_idx)
        T_mb = list(generated_time[i] - 1 for i in train_idx)
        Y_mb = list(np.reshape(generated_data_norm[i][1:, (dim - 1)],
                               [len(generated_data_norm[i][1:, (dim - 1)]), 1]) for i in train_idx)

        # Convert to tensors
        X_mb = torch.FloatTensor(np.array(X_mb)).to(device)
        Y_mb = torch.FloatTensor(np.array(Y_mb)).to(device)
        T_mb = torch.LongTensor(T_mb).cpu()

        # Forward pass
        y_pred = predictor(X_mb, T_mb)

        # Compute loss
        p_loss = criterion(y_pred, Y_mb)

        # Backward and optimize
        optimizer.zero_grad()
        p_loss.backward()
        optimizer.step()

    # Test the trained model on the original data (ä½¿ç”¨å½’ä¸€åŒ–çš„æ•°æ®)
    predictor.eval()
    with torch.no_grad():
        idx = np.random.permutation(len(ori_data_norm))
        train_idx = idx[:no]

        X_mb = list(ori_data_norm[i][:-1, :(dim - 1)] for i in train_idx)
        T_mb = list(ori_time[i] - 1 for i in train_idx)
        Y_mb = list(np.reshape(ori_data_norm[i][1:, (dim - 1)],
                               [len(ori_data_norm[i][1:, (dim - 1)]), 1]) for i in train_idx)

        # Convert to tensors
        X_mb = torch.FloatTensor(np.array(X_mb)).to(device)
        T_mb = torch.LongTensor(T_mb).cpu()

        # Prediction
        pred_Y_curr = predictor(X_mb, T_mb)
        pred_Y_curr = pred_Y_curr.cpu().numpy()

    # Compute the performance in terms of MAE
    MAE_temp = 0
    for i in range(no):
        MAE_temp = MAE_temp + mean_absolute_error(Y_mb[i], pred_Y_curr[i, :, :])

    predictive_score = MAE_temp / no

    print(f'  ğŸ“Š Predictive Score: {predictive_score:.4f}')

    return predictive_score