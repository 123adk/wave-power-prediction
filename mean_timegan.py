"""Time-series Generative Adversarial Networks (TimeGAN) Codebase - PyTorch Implementation

Reference: Jinsung Yoon, Daniel Jarrett, Mihaela van der Schaar,
"Time-series Generative Adversarial Networks,"
Neural Information Processing Systems (NeurIPS), 2019.

Paper link: https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks

Last updated Date: 2025-10-20
Converted to PyTorch by: GitHub Copilot
Updated by: 123adk - Added model and data saving functionality

-----------------------------

main_timegan_experiment.py (PyTorch Version with Save Functionality)

(1) Import data
(2) Generate synthetic data
(3) Save trained model and generated data
(4) Evaluate the performances in three ways
  - Visualization (t-SNE, PCA)
  - Discriminative score
  - Predictive score
"""

# Necessary packages
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import numpy as np
import warnings
import torch
import os
from datetime import datetime
import json

warnings.filterwarnings("ignore")

# 1. TimeGAN model
from timegan import timegan
# 2. Data loading
from data_loading import real_data_loading, sine_data_generation
# 3. Metrics
from metrics.discriminative_metrics import discriminative_score_metrics
from metrics.predictive_metrics import predictive_score_metrics
from metrics.visualization_metrics import visualization


def save_model_and_data(model_dict, generated_data, ori_data, args, metric_results, save_dir='./saved_models/wave'):
    """
    ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹ã€ç”Ÿæˆçš„æ•°æ®å’Œè¯„ä¼°ç»“æœ

    Args:
        - model_dict: æ¨¡å‹å­—å…¸ï¼ˆåŒ…å«æ‰€æœ‰ç½‘ç»œçš„state_dictï¼‰
        - generated_data: ç”Ÿæˆçš„åˆæˆæ•°æ®
        - ori_data: åŸå§‹æ•°æ®
        - args: å‘½ä»¤è¡Œå‚æ•°
        - metric_results: è¯„ä¼°æŒ‡æ ‡ç»“æœ
        - save_dir: ä¿å­˜ç›®å½•

    Returns:
        - saved_files: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å­—å…¸
    """
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)

    # ç”Ÿæˆæ—¶é—´æˆ³å’Œæ¨¡å‹åç§°
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    model_name = f'timegan_{args.data_name}_{timestamp}'

    # å®šä¹‰ä¿å­˜è·¯å¾„
    model_save_path = os.path.join(save_dir, f'{model_name}.pt')
    generated_data_path = os.path.join(save_dir, f'{model_name}_generated_data')
    original_data_path = os.path.join(save_dir, f'{model_name}_original_data')
    params_save_path = os.path.join(save_dir, f'{model_name}_parameters.json')
    metrics_save_path = os.path.join(save_dir, f'{model_name}_metrics.json')

    print('\n' + '=' * 70)
    print('ğŸ’¾ ä¿å­˜æ¨¡å‹å’Œæ•°æ®...')
    print('=' * 70)

    # 1. ä¿å­˜æ¨¡å‹
    try:
        torch.save(model_dict, model_save_path)
        print(f'âœ… æ¨¡å‹å·²ä¿å­˜: {model_save_path}')
        model_size = os.path.getsize(model_save_path) / (1024 * 1024)  # MB
        print(f'   æ–‡ä»¶å¤§å°: {model_size:.2f} MB')
    except Exception as e:
        print(f'âŒ æ¨¡å‹ä¿å­˜å¤±è´¥: {str(e)}')

    # 2. ä¿å­˜ç”Ÿæˆçš„æ•°æ®
    try:
        np.save(generated_data_path, generated_data)
        print(f'âœ… ç”Ÿæˆæ•°æ®å·²ä¿å­˜: {generated_data_path}')
        print(f'   æ•°æ®å½¢çŠ¶: {np.array(generated_data).shape}')
        data_size = os.path.getsize(generated_data_path) / (1024 * 1024)  # MB
        print(f'   æ–‡ä»¶å¤§å°: {data_size:.2f} MB')
    except Exception as e:
        print(f'âŒ ç”Ÿæˆæ•°æ®ä¿å­˜å¤±è´¥: {str(e)}')

    # 3. ä¿å­˜åŸå§‹æ•°æ®ï¼ˆç”¨äºåç»­å¯¹æ¯”ï¼‰
    try:
        np.save(original_data_path, ori_data)
        print(f'âœ… åŸå§‹æ•°æ®å·²ä¿å­˜: {original_data_path}')
        print(f'   æ•°æ®å½¢çŠ¶: {np.array(ori_data).shape}')
    except Exception as e:
        print(f'âŒ åŸå§‹æ•°æ®ä¿å­˜å¤±è´¥: {str(e)}')

    # 4. ä¿å­˜è®­ç»ƒå‚æ•°
    try:
        params_dict = {
            'data_name': args.data_name,
            'seq_len': args.seq_len,
            'module': args.module,
            'hidden_dim': args.hidden_dim,
            'num_layer': args.num_layer,
            'iterations': args.iteration,
            'batch_size': args.batch_size,
            'metric_iteration': args.metric_iteration,
            'timestamp': timestamp,
            'user': '123adk',
            'pytorch_version': torch.__version__,
            'cuda_available': torch.cuda.is_available(),
            'device': str(torch.cuda.get_device_name(0)) if torch.cuda.is_available() else 'CPU'
        }

        with open(params_save_path, 'w') as f:
            json.dump(params_dict, f, indent=4)
        print(f'âœ… è®­ç»ƒå‚æ•°å·²ä¿å­˜: {params_save_path}')
    except Exception as e:
        print(f'âŒ å‚æ•°ä¿å­˜å¤±è´¥: {str(e)}')

    # 5. ä¿å­˜è¯„ä¼°æŒ‡æ ‡
    try:
        metrics_dict = {
            'discriminative_score': float(metric_results['discriminative']),
            'predictive_score': float(metric_results['predictive']),
            'timestamp': timestamp,
            'data_name': args.data_name
        }

        with open(metrics_save_path, 'w') as f:
            json.dump(metrics_dict, f, indent=4)
        print(f'âœ… è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜: {metrics_save_path}')
    except Exception as e:
        print(f'âŒ è¯„ä¼°æŒ‡æ ‡ä¿å­˜å¤±è´¥: {str(e)}')

    print('=' * 70)
    print('âœ… æ‰€æœ‰æ–‡ä»¶ä¿å­˜å®Œæˆ!')
    print('=' * 70)

    # è¿”å›ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
    saved_files = {
        'model': model_save_path,
        'generated_data': generated_data_path,
        'original_data': original_data_path,
        'parameters': params_save_path,
        'metrics': metrics_save_path,
        'model_name': model_name
    }

    return saved_files


def print_summary(saved_files, metric_results, args):
    """
    æ‰“å°å®éªŒæ€»ç»“

    Args:
        - saved_files: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„å­—å…¸
        - metric_results: è¯„ä¼°æŒ‡æ ‡
        - args: å‘½ä»¤è¡Œå‚æ•°
    """
    print('\n' + '=' * 70)
    print('ğŸ“Š å®éªŒæ€»ç»“')
    print('=' * 70)
    print(f'å®éªŒåç§°: {saved_files["model_name"]}')
    print(f'æ•°æ®é›†: {args.data_name}')
    print(f'åºåˆ—é•¿åº¦: {args.seq_len}')
    print(f'è®­ç»ƒè¿­ä»£æ¬¡æ•°: {args.iteration}')
    print(f'æ‰¹æ¬¡å¤§å°: {args.batch_size}')
    print(f'\nğŸ“ˆ è¯„ä¼°æŒ‡æ ‡:')
    print(f'  â€¢ Discriminative Score: {metric_results["discriminative"]:.4f}')
    print(f'  â€¢ Predictive Score: {metric_results["predictive"]:.4f}')
    print(f'\nğŸ“ ä¿å­˜çš„æ–‡ä»¶:')
    for file_type, file_path in saved_files.items():
        if file_type != 'model_name':
            print(f'  â€¢ {file_type}: {file_path}')
    print('=' * 70)


def main(args):
    """Main function for TimeGAN experiments.

    Args:
        - data_name: sine, stock, or energy
        - seq_len: sequence length
        - Network parameters (should be optimized for different datasets)
          - module: gru, lstm, or lstmLN
          - hidden_dim: hidden dimensions
          - num_layer: number of layers
          - iteration: number of training iterations
          - batch_size: the number of samples in each batch
        - metric_iteration: number of iterations for metric computation

    Returns:
        - ori_data: original data
        - generated_data: generated synthetic data
        - metric_results: discriminative and predictive scores
        - saved_files: paths to saved files
    """

    print('=' * 70)
    print('ğŸš€ TimeGAN å®éªŒå¼€å§‹')
    print('=' * 70)
    ori_data=np.load('data/Z_train.npy')
    # ==================== 1. Data loading ====================
    print('\nğŸ“‚ åŠ è½½æ•°æ®...')
    #if args.data_name in ['stock', 'energy','wave']:
    #    ori_data = real_data_loading(args.data_name, args.seq_len)
    #elif args.data_name == 'sine':
        # Set number of samples and its dimensions
    #    no, dim = 10000, 5
    #    ori_data = sine_data_generation(no, args.seq_len, dim)

    print(f'âœ… {args.data_name} æ•°æ®é›†å·²åŠ è½½')
    print(f'   æ•°æ®å½¢çŠ¶: {np.array(ori_data).shape}')

    # ==================== 2. Synthetic data generation ====================
    print('\nğŸ”§ é…ç½®ç½‘ç»œå‚æ•°...')
    # Set network parameters
    parameters = dict()
    parameters['module'] = args.module
    parameters['hidden_dim'] = args.hidden_dim
    parameters['num_layer'] = args.num_layer
    parameters['iterations'] = args.iteration
    parameters['batch_size'] = args.batch_size

    # æ·»åŠ è®¾å¤‡é…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨ GPUï¼‰
    if torch.cuda.is_available():
        parameters['device'] = torch.device('cuda')
        print(f'âœ… ä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}')
    else:
        parameters['device'] = torch.device('cpu')
        print('âš ï¸  ä½¿ç”¨ CPU (å»ºè®®ä½¿ç”¨ GPU ä»¥åŠ é€Ÿè®­ç»ƒ)')

    print(f'\nâ³ å¼€å§‹è®­ç»ƒ TimeGAN...')
    print(f'   æ¨¡å—ç±»å‹: {args.module}')
    print(f'   éšè—å±‚ç»´åº¦: {args.hidden_dim}')
    print(f'   ç½‘ç»œå±‚æ•°: {args.num_layer}')
    print(f'   è¿­ä»£æ¬¡æ•°: {args.iteration}')
    print(f'   æ‰¹æ¬¡å¤§å°: {args.batch_size}')

    # è®­ç»ƒ TimeGAN å¹¶è·å–ç”Ÿæˆçš„æ•°æ®å’Œæ¨¡å‹
    generated_data, model_dict = timegan(ori_data, parameters)
    print('âœ… TimeGAN è®­ç»ƒå®Œæˆ!')

    # ==================== 3. Save model and data ====================
    # Performance metrics
    # Output initialization
    metric_results = dict()

    # å…ˆè¿›è¡Œè¯„ä¼°ï¼Œå†ä¿å­˜ï¼ˆè¿™æ ·å¯ä»¥æŠŠè¯„ä¼°ç»“æœä¸€èµ·ä¿å­˜ï¼‰
    print('\nğŸ“Š è¯„ä¼°ç”Ÿæˆæ•°æ®è´¨é‡...')

    # 1. Discriminative Score
    print(f'\n  è®¡ç®— Discriminative Score (è¿­ä»£ {args.metric_iteration} æ¬¡)...')
    discriminative_score = list()
    for i in range(args.metric_iteration):
        temp_disc = discriminative_score_metrics(ori_data, generated_data)
        discriminative_score.append(temp_disc)
        print(f'    è¿­ä»£ {i+1}/{args.metric_iteration}: {temp_disc:.4f}')

    metric_results['discriminative'] = np.mean(discriminative_score)
    print(f'  âœ… å¹³å‡ Discriminative Score: {metric_results["discriminative"]:.4f}')

    # 2. Predictive score
    print(f'\n  è®¡ç®— Predictive Score (è¿­ä»£ {args.metric_iteration} æ¬¡)...')
    predictive_score = list()
    for tt in range(args.metric_iteration):
        temp_pred = predictive_score_metrics(ori_data, generated_data)
        predictive_score.append(temp_pred)
        print(f'    è¿­ä»£ {tt+1}/{args.metric_iteration}: {temp_pred:.4f}')

    metric_results['predictive'] = np.mean(predictive_score)
    print(f'  âœ… å¹³å‡ Predictive Score: {metric_results["predictive"]:.4f}')

    # ==================== 4. Save everything ====================
    saved_files = save_model_and_data(
        model_dict=model_dict,
        generated_data=generated_data,
        ori_data=ori_data,
        args=args,
        metric_results=metric_results,
        save_dir=args.save_dir
    )

    # ==================== 5. Visualization ====================
    print('\nğŸ“ˆ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...')
    try:
        visualization(ori_data, generated_data, 'pca')
        print('  âœ… PCA å¯è§†åŒ–å®Œæˆ')
    except Exception as e:
        print(f'  âš ï¸  PCA å¯è§†åŒ–å¤±è´¥: {str(e)}')

    try:
        visualization(ori_data, generated_data, 'tsne')
        print('  âœ… t-SNE å¯è§†åŒ–å®Œæˆ')
    except Exception as e:
        print(f'  âš ï¸  t-SNE å¯è§†åŒ–å¤±è´¥: {str(e)}')

    # ==================== 6. Print summary ====================
    print_summary(saved_files, metric_results, args)

    return ori_data, generated_data, metric_results, saved_files


if __name__ == '__main__':
    # Inputs for the main function
    parser = argparse.ArgumentParser(
        description='TimeGAN - Time-series Generative Adversarial Networks (PyTorch Implementation)'
    )

    # Data parameters
    parser.add_argument(
        '--data_name',
        choices=['sine', 'stock', 'energy','wave'],
        default='wave',
        type=str,
        help='Dataset name: sine (synthetic), stock, or energy')

    parser.add_argument(
        '--seq_len',
        default=24,
        type=int,
        help='Sequence length of time-series data')

    # Network parameters
    parser.add_argument(
        '--module',
        choices=['gru', 'lstm', 'lstmLN'],
        default='gru',
        type=str,
        help='RNN module type: gru, lstm, or lstmLN')

    parser.add_argument(
        '--hidden_dim',
        default=24,
        type=int,
        help='Hidden state dimensions (should be optimized for different datasets)')

    parser.add_argument(
        '--num_layer',
        default=3,
        type=int,
        help='Number of RNN layers (should be optimized)')

    # Training parameters
    parser.add_argument(
        '--iteration',
        default=50000,
        type=int,
        help='Number of training iterations (should be optimized)')

    parser.add_argument(
        '--batch_size',
        default=128,
        type=int,
        help='Number of samples in mini-batch (should be optimized)')

    # Evaluation parameters
    parser.add_argument(
        '--metric_iteration',
        default=10,
        type=int,
        help='Number of iterations for metric computation')

    # Save parameters
    parser.add_argument(
        '--save_dir',
        default='./saved_models',
        type=str,
        help='Directory to save trained models and generated data')

    args = parser.parse_args()

    # Calls main function
    ori_data, generated_data, metrics, saved_files = main(args)

    print('\n' + '=' * 70)
    print('ğŸ‰ å®éªŒå®Œæˆ!')
    print('=' * 70)